---
version: '3'
services:
# Install elastic stack
  kibana:
    image: docker.elastic.co/kibana/kibana:${TAG}
    environment:
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
    ports: ['127.0.0.1:5601:5601']
    networks: ['stack']
    depends_on: ['elasticsearch']

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch-platinum:${TAG}
    environment: ['http.host=0.0.0.0', 'transport.host=127.0.0.1', 'ELASTIC_PASSWORD=${ELASTIC_PASSWORD}']
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks: ['stack']
    ports: ['127.0.0.1:9200:9200']

  logstash:
    image: logstash_kafka
    environment: ['http.host=0.0.0.0', 'transport.host=127.0.0.1', 'ELASTIC_PASSWORD=${ELASTIC_PASSWORD}']
    ports: ['127.0.0.1:5044:5044', '9600:9600', '7777:7777']
    build:

      context: ./dockerfiles/logstash
    # Provide a simple pipeline configuration for Logstash with a bind-mounted file.
    volumes:
      - ./local-mnt/:/mnt/
      - ./logstash.conf:/config-dir/logstash.conf
    restart: always
    command: bin/logstash -f /config-dir/logstash.conf
    links:
      - elasticsearch
      - kafka1
      - kafka2
      - kafka3
    networks: ['stack']

# Kafka implementation
  kafka1:
    image: wurstmeister/kafka
    depends_on:
      - zoo1
      - zoo2
      - zoo3
    links:
      - zoo1
      - zoo2
      - zoo3
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_LOG_RETENTION_HOURS: "168"
      KAFKA_LOG_RETENTION_BYTES: "100000000"
      KAFKA_ZOOKEEPER_CONNECT:  zoo1:2181,zoo2:2181,zoo3:2181
      KAFKA_CREATE_TOPICS: "log:3:3"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
    networks: ['stack']
  
  kafka2:
    image: wurstmeister/kafka
    depends_on:
      - zoo1
      - zoo2
      - zoo3
    links:
      - zoo1
      - zoo2
      - zoo3
    ports:
      - "9093:9092"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_LOG_RETENTION_HOURS: "168"
      KAFKA_LOG_RETENTION_BYTES: "100000000"
      KAFKA_ZOOKEEPER_CONNECT:  zoo1:2181,zoo2:2181,zoo3:2181
      KAFKA_CREATE_TOPICS: "log:3:3"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
    networks: ['stack']
  
  kafka3:
    image: wurstmeister/kafka
    depends_on:
      - zoo1
      - zoo2
      - zoo3
    links:
      - zoo1
      - zoo2
      - zoo3
    ports:
      - "9094:9092"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_LOG_RETENTION_HOURS: "168"
      KAFKA_LOG_RETENTION_BYTES: "100000000"
      KAFKA_ZOOKEEPER_CONNECT:  zoo1:2181,zoo2:2181,zoo3:2181
      KAFKA_CREATE_TOPICS: "log:3:3"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
    networks: ['stack']
  
  zoo1:
    image: elevy/zookeeper:latest
    environment:
      MYID: 1
      SERVERS: zoo1,zoo2,zoo3
    ports:
      - "2181:2181"
    networks: ['stack']
  
  zoo2:
    image: elevy/zookeeper:latest
    environment:
      MYID: 2
      SERVERS: zoo1,zoo2,zoo3
    ports:
      - "2182:2181"
    networks: ['stack']
  
  zoo3:
    image: elevy/zookeeper:latest
    environment:
      MYID: 3
      SERVERS: zoo1,zoo2,zoo3
    ports:
      - "2183:2181"
    networks: ['stack']

# Beats Setup  
  filebeat:
    image: filebeat_kafka
    environment: ['http.host=0.0.0.0', 'transport.host=127.0.0.1']
    # If the host system has logs at "/var/log", mount them at "/mnt/log"
    # inside the container, where Filebeat can find them.
    build:

      context: ./dockerfiles/filebeat
    volumes:
      - "./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro"
      - "./apache-logs:/apache-logs"
    links:
      - kafka1
      - kafka2
      - kafka3
    depends_on:
      - apache
      - kafka1
      - kafka2
      - kafka3
    networks: ['stack']
    depends_on: ['elasticsearch']

  apache:
    image: lzrbear/docker-apache2-ubuntu
    volumes:
      - "./apache-logs:/var/log/apache2"
    ports:
      - "8888:80"
    depends_on:
      - logstash
    networks: ['stack']

networks: {stack: {}}